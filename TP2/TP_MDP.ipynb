{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2 : \n",
    "## Algorithmes itératifs pour les processus de décision Markovien\n",
    "\n",
    "Dans ce TP2 sur les algorithmes itératifs pour les processus de décision Markovien, l'objectif est d'appliquer l'algorithme d'itération de la valeur pour déterminer la politique optimale dans un environnement de grille (gridworld). Ce problème met en évidence l'application des principes des processus de décision Markoviens (MDP) à un environnement simple mais illustratif."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définitions des fonctions :\n",
    "\n",
    "- Définition de la Fonction de Mouvement (move):\n",
    " Cette fonction Retourne les coordonnées de l'état après l'action.\n",
    "\n",
    "- Validation des Mouvements:\n",
    " La fonction is_valid_move vérifie si un mouvement proposé est valide, c'est-à-dire qu'il ne sort pas des limites de la grille et qu'il ne passe pas à travers un mur.\n",
    "\n",
    "- Actions Possibles:\n",
    " possible_actions retourne une liste des actions valides à partir d'une position donnée, en utilisant is_valid_move pour filtrer les actions non valides.\n",
    "\n",
    "- Rotation des Actions de 90 Degrés:\n",
    " action_90degree est une fonction destinée à identifier les actions perpendiculaires à une action donnée.\n",
    "\n",
    "- get_state_index :\n",
    " Fonction pour obtenir l'indice linéaire d'un état."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "\n",
    "def move (state, action):\n",
    "    x, y = state\n",
    "    if action == (1,0):\n",
    "        next_state = (x+1, y)\n",
    "    elif action == (-1,0):\n",
    "        next_state = (x-1, y)\n",
    "    elif action == (0,1):\n",
    "        next_state = (x, y+1)\n",
    "    elif action == (0,-1):\n",
    "        next_state = (x, y-1)\n",
    "    return next_state\n",
    "\n",
    "def is_valid_move(position, action):\n",
    "    new_position = move(position, action)\n",
    "    x, y = new_position\n",
    "    if 0 <= x < n_rows and 0 <= y < n_cols and new_position != wall:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def possible_actions(position):\n",
    "    valid_actions = []\n",
    "    if position != wall:\n",
    "        for action in actions:\n",
    "            if is_valid_move(position, action):\n",
    "                valid_actions.append(action)\n",
    "    return valid_actions\n",
    "\n",
    "def action_90degree (action,actions_possible) :\n",
    "    actions= []\n",
    "    for i in actions_possible :\n",
    "        if i != action:\n",
    "            if i+action != (0,0):\n",
    "                actions.append(i)\n",
    "    return actions\n",
    "\n",
    "def get_state_index(x, y):\n",
    "    return x * n_cols + y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation \n",
    "- Initialisation de la Table des Récompenses:\n",
    " On commence par initialiser une grille de récompenses avec des zéros et on définit des récompenses spécifiques pour certains états, comme un état objectif (goal) avec une récompense de 1 et un état pénalisant (bad) avec une récompense de -1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions possibles depuis la position  (0, 0) : [(1, 0), (0, 1)] \n",
      "Actions possibles depuis la position  (0, 1) : [(0, -1), (0, 1)] \n",
      "Actions possibles depuis la position  (0, 2) : [(1, 0), (0, -1), (0, 1)] \n",
      "Actions possibles depuis la position  (0, 3) : [(1, 0), (0, -1)] <== goal \n",
      "Actions possibles depuis la position  (1, 0) : [(1, 0), (-1, 0)] \n",
      "Actions possibles depuis la position  (1, 1) : [] <== wall \n",
      "Actions possibles depuis la position  (1, 2) : [(1, 0), (-1, 0), (0, 1)] \n",
      "Actions possibles depuis la position  (1, 3) : [(1, 0), (-1, 0), (0, -1)] <== bad \n",
      "Actions possibles depuis la position  (2, 0) : [(-1, 0), (0, 1)] \n",
      "Actions possibles depuis la position  (2, 1) : [(0, -1), (0, 1)] \n",
      "Actions possibles depuis la position  (2, 2) : [(-1, 0), (0, -1), (0, 1)] \n",
      "Actions possibles depuis la position  (2, 3) : [(-1, 0), (0, -1)] \n",
      "Table des récompenses initial:\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0. -1.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "+---+---+---+---+\n",
      "|   |   |   | G |\n",
      "+---+---+---+---+\n",
      "|   | W |   | B |\n",
      "+---+---+---+---+\n",
      "|   |   |   |   |\n",
      "+---+---+---+---+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_rows, n_cols = 3, 4 \n",
    "actions = [(1 ,0), (-1,0), (0,-1), (0,1)]\n",
    "goal = (0 , 3)  # Coordonnées de l'état objectif\n",
    "bad = (1 , 3)   # Coordonnées de l'état pénalisant\n",
    "wall = (1, 1)  # Position du mur (en indexation à partir de 0)\n",
    "\n",
    "n_states = n_rows * n_cols\n",
    "gamma = 0.9  # Facteur d'escompte\n",
    "threshold = 0.01  # Seuil de convergence\n",
    "V = np.zeros(n_states)\n",
    "V[get_state_index(0,3)]=1\n",
    "V[get_state_index(1,3)]=-1\n",
    "\n",
    "reward = np.zeros((n_rows, n_cols))\n",
    "reward[goal] = 1\n",
    "reward[bad] = -1\n",
    "\n",
    "# Parcourir la grille et afficher les actions possibles pour chaque case\n",
    "for x in range(n_rows):\n",
    "    for y in range(n_cols):\n",
    "        position = (x, y)\n",
    "        if position == goal:\n",
    "            exp = \"<== goal \"\n",
    "        elif position == bad:\n",
    "            exp = \"<== bad \"\n",
    "        elif position == wall:\n",
    "            exp = \"<== wall \"\n",
    "        else:\n",
    "            exp = \"\"\n",
    "        print(f\"Actions possibles depuis la position  {position} : {possible_actions(position)} {exp}\")\n",
    "\n",
    "\n",
    "# Affichage de la table des récompenses initiale\n",
    "print(\"Table des récompenses initial:\")\n",
    "print(V.reshape (n_rows, n_cols))\n",
    "\n",
    "def draw_grid_table(n_rows, n_cols, goal, bad, wall):\n",
    "    grid = [[' ' for _ in range(n_cols)] for _ in range(n_rows)]\n",
    "    grid[goal[0]][goal[1]] = 'G'\n",
    "    grid[bad[0]][bad[1]] = 'B'\n",
    "    grid[wall[0]][wall[1]] = 'W'\n",
    "    \n",
    "    for row in grid:\n",
    "        print('+---' * n_cols + '+')\n",
    "        print('| ' + ' | '.join(row) + ' |')\n",
    "    print('+---' * n_cols + '+')\n",
    "\n",
    "draw_grid_table(n_rows, n_cols, goal, bad, wall)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Itération de la Valeur:\n",
    " Au cœur de l'algorithme, on itère sur chaque état de la grille, en excluant le mur, l'état objectif et l'état pénalisant de la mise à jour puisque leur valeur est déjà définie. Pour chaque état restant, on calcule les récompenses attendues pour chaque action possible en tenant compte à la fois de l'action principale et des déviations possibles."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul de la Somme des Récompenses: Pour chaque action, on calcule la somme des récompenses attendues, en ajustant la probabilité en fonction du nombre d'actions de déviation possibles (0.1 pour deux déviations possibles, 0.2 pour une seule). Cette somme prend en compte la récompense immédiate de l'état, la valeur future escomptée des états accessibles à partir de l'action actuelle et des actions de déviation.\n",
    "Convergence: L'algorithme continue d'itérer jusqu'à ce que le changement maximal dans les valeurs des états entre deux itérations consécutives tombe en dessous d'un seuil de convergence prédéfini (threshold == 0.01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fonction de valeur optimale :\n",
      "[[ 0.59675907  0.68715851  0.78871585  1.        ]\n",
      " [ 0.51881065  0.          0.46787541 -1.        ]\n",
      " [ 0.45137749  0.39326998  0.34145044  0.04584432]]\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    delta = 0\n",
    "    for x in range(n_rows):\n",
    "        for y in range(n_cols):\n",
    "            if (x, y) == wall or (x,y) == goal  or (x,y) == bad:\n",
    "                continue  # Pas de calcul  pour ces cases\n",
    "            v = V[get_state_index(x, y)]\n",
    "            Vs = []\n",
    "            pa=possible_actions((x, y))\n",
    "            for action in pa:\n",
    "                for other_action in action_90degree(action,pa) :\n",
    "                    if len(action_90degree(action,pa)) == 1 :\n",
    "                        proba=0.2\n",
    "                    else :\n",
    "                        proba=0.1\n",
    "                    sum_derivations= proba * V[get_state_index(*move((x, y), other_action))]\n",
    "                sum_action = reward[x, y] + gamma * 0.8 * V[get_state_index(*move((x, y), action))]\n",
    "                sum_rewards = sum_action + sum_derivations\n",
    "                Vs.append(sum_rewards)\n",
    "            V[get_state_index(x, y)] = max(Vs)\n",
    "            delta = max(delta, abs(v - V[get_state_index(x, y)]))\n",
    "    if delta < threshold:\n",
    "        break\n",
    "\n",
    "print(\"Fonction de valeur optimale :\")\n",
    "print (V.reshape(n_rows, n_cols ))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résultat de la fonction de valeur optimale :\n",
    "\n",
    "|   1    |    2   |   3    | 4  |\n",
    "|-------|-------|-------|-------|\n",
    "| 0.60  | 0.69  | 0.79  | **Goal**  |\n",
    "| 0.52  | **Wall**  | 0.47  | **Bad**  |\n",
    "| 0.45  | 0.39  | 0.34  | 0.05  |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Politique optimale π* : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politique optimale π* :\n",
      "[[(0, 1) (0, 1) (0, 1) 'Goal']\n",
      " [(-1, 0) 'Wall' (-1, 0) 'Bad']\n",
      " [(-1, 0) (0, -1) (-1, 0) (0, -1)]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "π_star = np.full((n_rows, n_cols), None)\n",
    "\n",
    "for x in range(n_rows):\n",
    "    for y in range(n_cols):\n",
    "        # Exclure le goal et le bad et les murs\n",
    "        if (x, y) == goal or (x, y) == bad or (x, y) == wall:\n",
    "            continue\n",
    "\n",
    "        best_value = float('-inf')\n",
    "        best_action = None\n",
    "        pa = possible_actions((x, y))\n",
    "        for action in pa:\n",
    "            action_value = reward[x, y]\n",
    "            transition_states = action_90degree(action, pa)  # Les états possibles après avoir pris 'action'\n",
    "            action_prob = 0.8  # Probabilité de prendre l'action prévue\n",
    "            deviation_prob = (1 - action_prob) / len(transition_states) if transition_states else 0\n",
    "            \n",
    "            # Calculer la valeur attendue de prendre l'action\n",
    "            for other_action in transition_states:\n",
    "                next_state = move((x, y), other_action)\n",
    "                action_value += deviation_prob * V[get_state_index(*next_state)]\n",
    "            \n",
    "            # Ajouter la valeur attendue si l'action principale est exécutée avec succès\n",
    "            next_state = move((x, y), action)\n",
    "            action_value += action_prob * V[get_state_index(*next_state)]\n",
    "            \n",
    "            # Si la valeur calculée pour cette action est meilleure, la retenir\n",
    "            if action_value > best_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        \n",
    "        π_star[x, y] = best_action  # Mettre à jour la politique optimale avec la meilleure action\n",
    "\n",
    "# Mettre des strings pour le goal et le bad dans la politique optimale\n",
    "π_star[goal[0], goal[1]] = \"Goal\"\n",
    "π_star[bad[0], bad[1]] = \"Bad\"\n",
    "π_star[ wall[0], wall[1]] = \"Wall\"\n",
    "\n",
    "print(\"Politique optimale π* :\")\n",
    "print(π_star)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meilleur Action à chaque position :\n",
    "\n",
    "|     1      |     2      |     3      | 4      |\n",
    "|-----------|-----------|-----------|-----------|\n",
    "| Droite    | Droite    | Droite    | **Goal**     |\n",
    "| Haut      | **Wall**   | Haut      | **Bad**|\n",
    "| Haut      | Gauche    | Haut      | Gauche    |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
